{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "136b311b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T22:31:34.585751500Z",
     "start_time": "2024-01-03T22:31:34.581748400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theal\\PycharmProjects\\stockgym\\src\\agents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\theal\\\\PycharmProjects')\n",
    "sys.path.append('C:\\\\Users\\\\theal\\\\PycharmProjects\\\\stockgym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28164fa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-03T22:31:36.954647600Z",
     "start_time": "2024-01-03T22:31:36.421365300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from stockgym.src.envs.gym_up_and_to_the_right.up_and_to_the_right_env import UpAndToTheRightEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1942223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward from the episode 1: 13.583092541572421\n",
      "Total reward from the episode 2: -59.60287902968517\n",
      "Total reward from the episode 3: 38.41056528315692\n",
      "Total reward from the episode 4: -16.34824966865538\n",
      "Total reward from the episode 5: 2.843988446780956\n",
      "Total reward from the episode 6: -56.89730099175364\n",
      "Total reward from the episode 7: 2.6848243277873562\n",
      "Total reward from the episode 8: -41.752438634073684\n",
      "Total reward from the episode 9: -17.645901032367405\n",
      "Total reward from the episode 10: -50.43563122853185\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class RandomActionAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def get_action(self):\n",
    "        valid_actions = self.env.controller.get_valid_actions()\n",
    "        action = np.random.choice(valid_actions)\n",
    "        return action\n",
    "\n",
    "    def run_episode(self):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.get_action()\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "# Create the environment\n",
    "env = UpAndToTheRightEnv(state_type=\"Basic\", \n",
    "                         reward_type=\"FinalOnly\",\n",
    "                         num_prev_obvs=5,\n",
    "                         price_movement_type=\"Linear\",\n",
    "                         offset_scaling=1.0,\n",
    "                         scale=1.0,\n",
    "                         slope=1.0,\n",
    "                         noise=0.1,\n",
    "                         starting_price=100,\n",
    "                         num_steps=100,\n",
    "                         multiple_units=True,\n",
    "                         render=False)\n",
    "\n",
    "# Create and run the agent\n",
    "agent = RandomActionAgent(env)\n",
    "\n",
    "for i in range(10):\n",
    "    total_reward = agent.run_episode()\n",
    "    print(f\"Total reward from the episode {i + 1}: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079eb070",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=.1, discount_factor=0.999, exploration_rate=1, exploration_decay=.99):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.q_table = {}  # Initialize an empty Q-table\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state_key = self.state_to_key(state)\n",
    "        valid_actions = self.env.controller.get_valid_actions()\n",
    "        self.initialize_state_in_q_table(state_key)\n",
    "\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            return random.choice(valid_actions)  # Explore\n",
    "        else:\n",
    "            # Exploit by selecting the best valid action based on Q-values\n",
    "            q_values = np.array([self.q_table[state_key][a] for a in range(self.env.action_space.n)])\n",
    "            # Mask q_values of invalid actions with a very low number\n",
    "            mask = np.ones(len(q_values)) * -np.inf\n",
    "            mask[valid_actions] = 0\n",
    "            q_values_masked = q_values + mask\n",
    "            return np.argmax(q_values_masked)  # Exploit\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        state_key = self.state_to_key(state)\n",
    "        next_state_key = self.state_to_key(next_state)\n",
    "\n",
    "        # Ensure both the current and next states are in the Q-table\n",
    "        self.initialize_state_in_q_table(state_key)\n",
    "        self.initialize_state_in_q_table(next_state_key)\n",
    "\n",
    "        # Find the best next action from Q-table\n",
    "        best_next_action = np.argmax(self.q_table[next_state_key])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state_key][best_next_action]\n",
    "        td_error = td_target - self.q_table[state_key][action]\n",
    "        self.q_table[state_key][action] += self.learning_rate * td_error\n",
    "\n",
    "\n",
    "    def initialize_state_in_q_table(self, state_key):\n",
    "        if state_key not in self.q_table:\n",
    "            self.q_table[state_key] = [0 for _ in range(self.env.action_space.n)]\n",
    "\n",
    "\n",
    "    def state_to_key(self, state):\n",
    "        return tuple(state)\n",
    "\n",
    "\n",
    "    def run_episode(self):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            self.update_q_table(state, action, reward, next_state)\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        return total_reward\n",
    "    \n",
    "    def run_episode_w_render(self):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            self.update_q_table(state, action, reward, next_state)\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            self.env.render()\n",
    "            time.sleep(1)\n",
    "\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ddef455",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UpAndToTheRightEnv(state_type=\"Basic\", \n",
    "                         reward_type=\"FinalOnly\",\n",
    "                         num_prev_obvs=5,\n",
    "                         price_movement_type=\"Linear\",\n",
    "                         offset_scaling=1.0,\n",
    "                         scale=1.0,\n",
    "                         slope=1.0,\n",
    "                         noise=0.1,\n",
    "                         starting_price=100,\n",
    "                         num_steps=100,\n",
    "                         multiple_units=True,\n",
    "                         render=True)\n",
    "\n",
    "agent = QLearningAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31b2e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward from the episode 1: -14.0900322542956\n",
      "Total reward from the episode 2: 85.26661319501335\n",
      "Total reward from the episode 3: 36.63999018780939\n",
      "Total reward from the episode 4: 21.667132839599155\n",
      "Total reward from the episode 5: -131.70016943196237\n",
      "Total reward from the episode 6: 13.377931901463215\n",
      "Total reward from the episode 7: 23.72882952961703\n",
      "Total reward from the episode 8: -2.9218221266281716\n",
      "Total reward from the episode 9: -8.083957743698889\n",
      "Total reward from the episode 10: 63.8240008554983\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    total_reward = agent.run_episode()\n",
    "    print(f\"Total reward from the episode {i + 1}: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "157016b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Given implementation shouldn't be here",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episode_w_render\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 78\u001b[0m, in \u001b[0;36mQLearningAgent.run_episode_w_render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     77\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_rate \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_decay\n",
      "File \u001b[1;32m~\\PycharmProjects\\stockgym\\src\\envs\\gym_up_and_to_the_right\\up_and_to_the_right_env.py:65\u001b[0m, in \u001b[0;36mUpAndToTheRightEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontroller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\stockgym\\src\\envs\\stock\\controller.py:145\u001b[0m, in \u001b[0;36mController.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mupdate_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrader\u001b[38;5;241m.\u001b[39mprice_list[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrader\u001b[38;5;241m.\u001b[39maction_list)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven implementation shouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    146\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n",
      "\u001b[1;31mValueError\u001b[0m: Given implementation shouldn't be here"
     ]
    }
   ],
   "source": [
    "total_reward = agent.run_episode_w_render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d8b377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3374.5160847884927\n"
     ]
    }
   ],
   "source": [
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977edb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0.01,): [366.2202456951612, 11504.881429360406, 1087.6490088514588, 0, 0],\n",
       " (0.01, 1.0): [29.64955190324724,\n",
       "  780.5633847142296,\n",
       "  11716.90688103198,\n",
       "  249.8519792659809,\n",
       "  46.93757576578879],\n",
       " (0.01, 0.505, 1.0): [21.82343006816371,\n",
       "  948.9885988275026,\n",
       "  11929.976707023152,\n",
       "  215.71586494251707,\n",
       "  4.250745122399426],\n",
       " (0.01, 0.33999999999999997, 0.67, 1.0): [50.70541269358468,\n",
       "  485.4321024852635,\n",
       "  12143.66914519927,\n",
       "  743.48219152789,\n",
       "  15.635010471951222],\n",
       " (0.01, 0.2575, 0.505, 0.7525, 1.0): [12359.715360250812,\n",
       "  1971.0868154992734,\n",
       "  7437.656008919959,\n",
       "  12143.66914519927,\n",
       "  6930.710762133917]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a2f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d991a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env, learning_rate=0.001, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.995, min_exploration_rate=0.01, memory_size=100000, batch_size=64):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.min_exploration_rate = min_exploration_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Builds a neural network for approximating Q-values.\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Updates the target network.\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        valid_actions = self.env.controller.get_valid_actions()\n",
    "        state = self.pad_state(state)  # Assuming state padding as discussed earlier\n",
    "\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return np.random.choice(valid_actions)  # Explore\n",
    "        else:\n",
    "            q_values = self.model.predict(state)[0]\n",
    "            # Mask q_values of invalid actions with a very low number\n",
    "            mask = -np.inf * np.ones(len(q_values))\n",
    "            mask[valid_actions] = 0\n",
    "            q_values_masked = q_values + mask\n",
    "            return np.argmax(q_values_masked)  # Exploit\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Stores experience in replay memory.\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Trains the model using randomly sampled experiences from the memory.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.discount_factor * np.amax(self.target_model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "        if self.exploration_rate > self.min_exploration_rate:\n",
    "            self.exploration_rate *= self.exploration_decay\n",
    "            \n",
    "    def pad_state(self, state):\n",
    "        \"\"\"Pads the state vector with zeros if its length is less than the expected state size.\"\"\"\n",
    "        if not isinstance(state, np.ndarray):\n",
    "            state = np.array(state)\n",
    "\n",
    "        if state.ndim > 1:\n",
    "            state = state.flatten()  # Flatten the state if it's not already 1D\n",
    "\n",
    "        padded_state = np.zeros(self.state_size)\n",
    "        start_index = self.state_size - len(state)\n",
    "        padded_state[start_index:] = state\n",
    "        return np.reshape(padded_state, [1, self.state_size])\n",
    "\n",
    "\n",
    "    def run_episode(self):\n",
    "        state = self.env.reset()\n",
    "        state = self.pad_state(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = self.get_action(state)\n",
    "            print(action)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            next_state = self.pad_state(next_state)\n",
    "            self.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            self.replay()\n",
    "\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        return total_reward\n",
    "\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8afd0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9cfc64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "2\n",
      "0\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "4\n",
      "0\n",
      "2\n",
      "4\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "4\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "4\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "4\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "Total reward from the episode 1: -20.737491436185024\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     total_reward \u001b[38;5;241m=\u001b[39m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal reward from the episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 103\u001b[0m, in \u001b[0;36mDQNAgent.run_episode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    101\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m    102\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_rate \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_decay\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_reward\n",
      "Cell \u001b[1;32mIn[27], line 69\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m     target_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(state)\n\u001b[0;32m     68\u001b[0m     target_f[\u001b[38;5;241m0\u001b[39m][action] \u001b[38;5;241m=\u001b[39m target\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_rate \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_exploration_rate:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_rate \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_decay\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:1163\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 1163\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_counter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_train_begin()\n\u001b[0;32m   1165\u001b[0m training_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:903\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    898\u001b[0m     tensor_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    899\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    900\u001b[0m       (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot assign to variable\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m due to variable shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and value \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are incompatible\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    902\u001b[0m       (tensor_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, value_tensor\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m--> 903\u001b[0m assign_op \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_value:\n\u001b[0;32m    906\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_read(assign_op)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:140\u001b[0m, in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    139\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAssignVariableOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    143\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    total_reward = dqn_agent.run_episode()\n",
    "    print(f\"Total reward from the episode {i + 1}: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62be3c61",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQNAgent\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BoltzmannQPolicy\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequentialMemory\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rl'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# Assuming env is your Gym environment\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model\n",
    "\n",
    "model = build_model(states, actions)\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ba175",
   "metadata": {},
   "outputs": [],
   "source": [
    " from stable_baselines3 import PPO\n",
    "import gym\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('YourCustomEnv-v0')  # Replace with your environment\n",
    "\n",
    "# Create and train the model\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
